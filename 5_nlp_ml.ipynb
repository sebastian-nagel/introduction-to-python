{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "applicable-newsletter",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Text Processing and Machine Learning\n",
    "====================================\n",
    "\n",
    "- pre-processing and tokenization (splitting text into words)\n",
    "- n-grams, vectorization and word embeddings\n",
    "- train and evaluate a text classifier\n",
    "- a short look into [Hugging Face's transformers library](https://huggingface.co/transformers/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-community",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "[Natural language processing (NLP)](https://en.wikipedia.org/wiki/Natural_language_processing) is about programming computers to process and analyze natural language data (text and speech).\n",
    "\n",
    "During the text classification training we touch only some aspects of NLP, namely\n",
    "\n",
    "- tokenization or splitting a text into words (aka. tokens)\n",
    "- the representation of words in a vector space (word embeddings)\n",
    "\n",
    "NLP modules for Python:\n",
    "\n",
    "- [spaCy](https://spacy.io/) ([spaCy on pypi](https://pypi.org/project/spacy/))\n",
    "- [NLTK](https://www.nltk.org/) ([NLTK on pypi](https://pypi.org/project/nltk/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-casting",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning\n",
    "\n",
    "The field of machine learning is too broad to be introduced here. Please, see [Google's machine learning crash course](https://developers.google.com/machine-learning/crash-course/ml-intro)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-advance",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## fastText\n",
    "\n",
    "[fastText](https://fasttext.cc/) is a software library for text\n",
    "classification and word representation learning. See the fastText\n",
    "tutorials for\n",
    "\n",
    "- [text classification](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "- [word representation learning](https://fasttext.cc/docs/en/unsupervised-tutorial.html)\n",
    "\n",
    "We will now follow the [fastText text\n",
    "classification](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "tutorial (cf. documentation of the [Python module\n",
    "\"fasttext\"](https://pypi.org/project/fasttext/)) to train and apply\n",
    "a text classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-activity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The fastText tutorial uses the StackExchange cooking data set. We will use the [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview) data set. In order to download the data set, you need to register at [Kaggle.com](https://www.kaggle.com/).\n",
    "\n",
    "After the data set is downloaded and unpacked into the folder `data/kaggle-jigsaw-toxic`, you should see the tree files `train.csv`, `test.csv` and `test_labels.csv` in the mentioned folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "excess-program",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv('data/kaggle-jigsaw-toxic/train.csv')\n",
    "\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "green-tolerance",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.095844\n",
       "severe_toxic     0.009996\n",
       "obscene          0.052948\n",
       "threat           0.002996\n",
       "insult           0.049364\n",
       "identity_hate    0.008805\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train[labels].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-zealand",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Only 10% of the comments are toxic. What does it mean for building a classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "inside-christianity",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you're a hero http://example.com/index.html\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the comments\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(reduce_len=True)\n",
    "\n",
    "def tokenize(text):\n",
    "    global tweet_tokenizer\n",
    "    words = tweet_tokenizer.tokenize(text)\n",
    "    words = filter(lambda w: w != ''\n",
    "                             and w not in string.punctuation, words)\n",
    "    words = map(lambda w: w.lower(), words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "tokenize(\"You're a hero! http://example.com/index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guilty-combining",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# write data to fastText train file\n",
    "\n",
    "train_file = 'data/kaggle-jigsaw-toxic/train.txt'\n",
    "\n",
    "def write_line_fasttext(fp, row):\n",
    "    global labels\n",
    "    line = ''\n",
    "    for label in labels:\n",
    "        if row[label] == 1:\n",
    "            if line:\n",
    "                line += ' '\n",
    "            line += '__label__' + label\n",
    "    if line:\n",
    "        line += ' '\n",
    "    else:\n",
    "        line += '__label__none '\n",
    "    line += tokenize(row['comment_text'])\n",
    "    fp.write(line)\n",
    "    fp.write('\\n')\n",
    "\n",
    "with open(train_file, 'w') as fp:\n",
    "    df_train.apply(lambda row: write_line_fasttext(fp, row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varying-posting",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# train a model\n",
    "\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.train_supervised(input=train_file, wordNgrams=2, minCount=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "external-garbage",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__none',), array([0.99993789]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenize(\"This is a well-written article.\"))\n",
    "# model.predict(tokenize(\"Fuck you!\"), k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dominant-knife",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9997914433479309, 'stupid'),\n",
       " (0.9996288418769836, 'moron'),\n",
       " (0.9995864033699036, 'jerk'),\n",
       " (0.9993796348571777, 'arrogant'),\n",
       " (0.9993292093276978, 'ignorant'),\n",
       " (0.999278724193573, 'stupidity'),\n",
       " (0.9992066025733948, 'coward'),\n",
       " (0.9992029070854187, 'disgusting'),\n",
       " (0.9991973638534546, 'idiotic'),\n",
       " (0.9990672469139099, 'pathetic'),\n",
       " (0.9990224242210388, 'fool'),\n",
       " (0.9989080429077148, 'morons'),\n",
       " (0.9989030957221985, 'losers'),\n",
       " (0.9988322854042053, 'hell'),\n",
       " (0.9988279342651367, 'jackass'),\n",
       " (0.9987922310829163, 'fascist'),\n",
       " (0.9987281560897827, 'idiots'),\n",
       " (0.9987263679504395, 'dirty'),\n",
       " (0.9987045526504517, 'sucked'),\n",
       " (0.998673141002655, 'bloody')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking into the underlying word embeddings\n",
    "\n",
    "model.get_nearest_neighbors('idiot', k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "still-forward",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "model_file = 'data/kaggle-jigsaw-toxic/model.bin'\n",
    "\n",
    "model.save_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "determined-mumbai",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/kaggle-jigsaw-toxic/test.csv')\n",
    "df_test_labels = pd.read_csv('data/kaggle-jigsaw-toxic/test_labels.csv')\n",
    "\n",
    "# join both tables\n",
    "df_test = df_test.merge(df_test_labels, on='id')\n",
    "\n",
    "# skip rows not labelled / not used\n",
    "df_test = df_test[df_test['toxic'] != -1]\n",
    "\n",
    "test_file = 'data/kaggle-jigsaw-toxic/test.txt'\n",
    "\n",
    "# write test set for fastText\n",
    "with open(test_file, 'w') as fp:\n",
    "    df_test.apply(lambda row: write_line_fasttext(fp, row), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-phrase",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Validation\n",
    "\n",
    "See also: [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handled-carpet",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63978, 0.9303666885491888, 0.8240416430163499)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "assured-malawi",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('__label__threat', {'precision': nan, 'recall': 0.0, 'f1score': 0.0})\n",
      "('__label__identity_hate', {'precision': nan, 'recall': 0.0, 'f1score': 0.0})\n",
      "('__label__severe_toxic', {'precision': 0.275, 'recall': 0.05994550408719346, 'f1score': 0.09843400447427293})\n",
      "('__label__insult', {'precision': 0.7333333333333333, 'recall': 0.0032098044937262913, 'f1score': 0.006391632771644393})\n",
      "('__label__obscene', {'precision': 0.9406952965235174, 'recall': 0.12462747222974803, 'f1score': 0.22009569377990432})\n",
      "('__label__toxic', {'precision': 0.5887384176764077, 'recall': 0.6781609195402298, 'f1score': 0.6302937809996184})\n",
      "('__label__none', {'precision': 0.9737668280742829, 'recall': 0.950896336710834, 'f1score': 0.9621956990378043})\n"
     ]
    }
   ],
   "source": [
    "res_per_label = model.test_label(test_file)\n",
    "\n",
    "for label in res_per_label.items():\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "charitable-worst",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score : 0.962196  Precision : 0.973767  Recall : 0.950896   __label__none\r\n",
      "F1-Score : 0.630294  Precision : 0.588738  Recall : 0.678161   __label__toxic\r\n",
      "F1-Score : 0.220096  Precision : 0.940695  Recall : 0.124627   __label__obscene\r\n",
      "F1-Score : 0.006392  Precision : 0.733333  Recall : 0.003210   __label__insult\r\n",
      "F1-Score : 0.098434  Precision : 0.275000  Recall : 0.059946   __label__severe_toxic\r\n",
      "F1-Score : 0.000000  Precision : --------  Recall : 0.000000   __label__identity_hate\r\n",
      "F1-Score : 0.000000  Precision : --------  Recall : 0.000000   __label__threat\r\n",
      "N\t63978\r\n",
      "P@1\t0.930\r\n",
      "R@1\t0.824\r\n"
     ]
    }
   ],
   "source": [
    "# the fastText command-line tool has a nice output formatter\n",
    "!fasttext test-label \\\n",
    "   data/kaggle-jigsaw-toxic/model.bin \\\n",
    "   data/kaggle-jigsaw-toxic/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-bankruptcy",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformers\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
    "- [Hugging Face's transformers library](https://huggingface.co/transformers/): unique interface and provisioning of various transformer language models\n",
    "  - see https://huggingface.co/course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vulnerable-canada",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "!pip install transformers\n",
    "!pip install tensorflow\n",
    "!pip install \"transformers[sentencepiece]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liberal-essex",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "p = pipeline('fill-mask', model='bert-base-german-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "tutorial-initial",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Er arbeitet als Rechtsanwalt.', 'score': 0.09919334203004837, 'token': 6143, 'token_str': 'Rechtsanwalt'}\n",
      "{'sequence': 'Er arbeitet als Trainer.', 'score': 0.07836302369832993, 'token': 3674, 'token_str': 'Trainer'}\n",
      "{'sequence': 'Er arbeitet als Journalist.', 'score': 0.0628521665930748, 'token': 10486, 'token_str': 'Journalist'}\n",
      "{'sequence': 'Er arbeitet als Anwalt.', 'score': 0.05725342780351639, 'token': 6938, 'token_str': 'Anwalt'}\n",
      "{'sequence': 'Er arbeitet als Schauspieler.', 'score': 0.05046413466334343, 'token': 5607, 'token_str': 'Schauspieler'}\n"
     ]
    }
   ],
   "source": [
    "for s in p(\"Er arbeitet als [MASK].\"): print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "biblical-kinase",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pipeline_fill_mask = pipeline('fill-mask', model='bert-base-german-cased')\n",
    "\n",
    "def fill_mask(cloze):\n",
    "    global pipeline_fill_mask\n",
    "    for s in pipeline_fill_mask(cloze):\n",
    "        print('%-20s\\t%.5f' % (s['token_str'], s['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "understanding-association",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arzt                \t0.61843\n",
      "Angestellter        \t0.04225\n",
      "Koch                \t0.03064\n",
      "Assistent           \t0.02001\n",
      "Mediziner           \t0.01900\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Er arbeitet als [MASK] in einer Klinik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "funded-cotton",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arzt                \t0.69560\n",
      "Angestellter        \t0.03423\n",
      "Chemiker            \t0.02711\n",
      "Facharzt            \t0.02113\n",
      "Mediziner           \t0.02024\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Er arbeitet als [MASK] in einer Lungenklinik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "pending-linux",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingenieur           \t0.18871\n",
      "Berater             \t0.17160\n",
      "Manager             \t0.15090\n",
      "Geschäftsführer     \t0.07775\n",
      "Trainer             \t0.04951\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Er arbeitet als [MASK] bei BMW.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "driving-champagne",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor           \t0.74687\n",
      "Dozent              \t0.11445\n",
      "Hochschullehrer     \t0.08565\n",
      "Wissenschaftler     \t0.00667\n",
      "Assistent           \t0.00427\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Er arbeitet als [MASK] an der Universität Konstanz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "painted-trunk",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Professor           \t0.52318\n",
      "Lehrerin            \t0.09859\n",
      "Dozent              \t0.08542\n",
      "Professur           \t0.04144\n",
      "Richterin           \t0.02292\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Sie arbeitet als [MASK] an der Universität Konstanz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "hybrid-economy",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schön               \t0.11005\n",
      "jung                \t0.06098\n",
      "glücklich           \t0.05704\n",
      "toll                \t0.05053\n",
      "gut                 \t0.03495\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Sie ist wirklich [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "authorized-intention",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gut                 \t0.05452\n",
      "glücklich           \t0.05183\n",
      "da                  \t0.03765\n",
      "jung                \t0.03233\n",
      "tot                 \t0.03229\n"
     ]
    }
   ],
   "source": [
    "fill_mask(\"Er ist wirklich [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "close-instrument",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pipeline in module transformers.pipelines:\n",
      "\n",
      "pipeline(task: str, model: Optional = None, config: Union[str, transformers.configuration_utils.PretrainedConfig, NoneType] = None, tokenizer: Union[str, transformers.tokenization_utils.PreTrainedTokenizer, NoneType] = None, feature_extractor: Union[str, ForwardRef('SequenceFeatureExtractor'), NoneType] = None, framework: Optional[str] = None, revision: Optional[str] = None, use_fast: bool = True, use_auth_token: Union[str, bool, NoneType] = None, model_kwargs: Dict[str, Any] = {'use_auth_token': None}, **kwargs) -> transformers.pipelines.base.Pipeline\n",
      "    Utility factory method to build a :class:`~transformers.Pipeline`.\n",
      "    \n",
      "    Pipelines are made of:\n",
      "    \n",
      "        - A :doc:`tokenizer <tokenizer>` in charge of mapping raw textual input to token.\n",
      "        - A :doc:`model <model>` to make predictions from the inputs.\n",
      "        - Some (optional) post processing for enhancing model's output.\n",
      "    \n",
      "    Args:\n",
      "        task (:obj:`str`):\n",
      "            The task defining which pipeline will be returned. Currently accepted tasks are:\n",
      "    \n",
      "            - :obj:`\"feature-extraction\"`: will return a :class:`~transformers.FeatureExtractionPipeline`.\n",
      "            - :obj:`\"text-classification\"`: will return a :class:`~transformers.TextClassificationPipeline`.\n",
      "            - :obj:`\"sentiment-analysis\"`: (alias of :obj:`\"text-classification\") will return a\n",
      "              :class:`~transformers.TextClassificationPipeline`.\n",
      "            - :obj:`\"token-classification\"`: will return a :class:`~transformers.TokenClassificationPipeline`.\n",
      "            - :obj:`\"ner\"` (alias of :obj:`\"token-classification\"): will return a\n",
      "              :class:`~transformers.TokenClassificationPipeline`.\n",
      "            - :obj:`\"question-answering\"`: will return a :class:`~transformers.QuestionAnsweringPipeline`.\n",
      "            - :obj:`\"fill-mask\"`: will return a :class:`~transformers.FillMaskPipeline`.\n",
      "            - :obj:`\"summarization\"`: will return a :class:`~transformers.SummarizationPipeline`.\n",
      "            - :obj:`\"translation_xx_to_yy\"`: will return a :class:`~transformers.TranslationPipeline`.\n",
      "            - :obj:`\"text2text-generation\"`: will return a :class:`~transformers.Text2TextGenerationPipeline`.\n",
      "            - :obj:`\"text-generation\"`: will return a :class:`~transformers.TextGenerationPipeline`.\n",
      "            - :obj:`\"zero-shot-classification:`: will return a :class:`~transformers.ZeroShotClassificationPipeline`.\n",
      "            - :obj:`\"conversational\"`: will return a :class:`~transformers.ConversationalPipeline`.\n",
      "        model (:obj:`str` or :obj:`~transformers.PreTrainedModel` or :obj:`~transformers.TFPreTrainedModel`, `optional`):\n",
      "            The model that will be used by the pipeline to make predictions. This can be a model identifier or an\n",
      "            actual instance of a pretrained model inheriting from :class:`~transformers.PreTrainedModel` (for PyTorch)\n",
      "            or :class:`~transformers.TFPreTrainedModel` (for TensorFlow).\n",
      "    \n",
      "            If not provided, the default for the :obj:`task` will be loaded.\n",
      "        config (:obj:`str` or :obj:`~transformers.PretrainedConfig`, `optional`):\n",
      "            The configuration that will be used by the pipeline to instantiate the model. This can be a model\n",
      "            identifier or an actual pretrained model configuration inheriting from\n",
      "            :class:`~transformers.PretrainedConfig`.\n",
      "    \n",
      "            If not provided, the default configuration file for the requested model will be used. That means that if\n",
      "            :obj:`model` is given, its default configuration will be used. However, if :obj:`model` is not supplied,\n",
      "            this :obj:`task`'s default model's config is used instead.\n",
      "        tokenizer (:obj:`str` or :obj:`~transformers.PreTrainedTokenizer`, `optional`):\n",
      "            The tokenizer that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained tokenizer inheriting from :class:`~transformers.PreTrainedTokenizer`.\n",
      "    \n",
      "            If not provided, the default tokenizer for the given :obj:`model` will be loaded (if it is a string). If\n",
      "            :obj:`model` is not specified or not a string, then the default tokenizer for :obj:`config` is loaded (if\n",
      "            it is a string). However, if :obj:`config` is also not given or not a string, then the default tokenizer\n",
      "            for the given :obj:`task` will be loaded.\n",
      "        feature_extractor (:obj:`str` or :obj:`~transformers.PreTrainedFeatureExtractor`, `optional`):\n",
      "            The feature extractor that will be used by the pipeline to encode data for the model. This can be a model\n",
      "            identifier or an actual pretrained feature extractor inheriting from\n",
      "            :class:`~transformers.PreTrainedFeatureExtractor`.\n",
      "    \n",
      "            Feature extractors are used for non-NLP models, such as Speech or Vision models as well as multi-modal\n",
      "            models. Multi-modal models will also require a tokenizer to be passed.\n",
      "    \n",
      "            If not provided, the default feature extractor for the given :obj:`model` will be loaded (if it is a\n",
      "            string). If :obj:`model` is not specified or not a string, then the default feature extractor for\n",
      "            :obj:`config` is loaded (if it is a string). However, if :obj:`config` is also not given or not a string,\n",
      "            then the default feature extractor for the given :obj:`task` will be loaded.\n",
      "        framework (:obj:`str`, `optional`):\n",
      "            The framework to use, either :obj:`\"pt\"` for PyTorch or :obj:`\"tf\"` for TensorFlow. The specified framework\n",
      "            must be installed.\n",
      "    \n",
      "            If no framework is specified, will default to the one currently installed. If no framework is specified and\n",
      "            both frameworks are installed, will default to the framework of the :obj:`model`, or to PyTorch if no model\n",
      "            is provided.\n",
      "        revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      "            When passing a task name or a string model identifier: The specific model version to use. It can be a\n",
      "            branch name, a tag name, or a commit id, since we use a git-based system for storing models and other\n",
      "            artifacts on huggingface.co, so ``revision`` can be any identifier allowed by git.\n",
      "        use_fast (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
      "            Whether or not to use a Fast tokenizer if possible (a :class:`~transformers.PreTrainedTokenizerFast`).\n",
      "        use_auth_token (:obj:`str` or `bool`, `optional`):\n",
      "            The token to use as HTTP bearer authorization for remote files. If :obj:`True`, will use the token\n",
      "            generated when running :obj:`transformers-cli login` (stored in :obj:`~/.huggingface`).\n",
      "            revision(:obj:`str`, `optional`, defaults to :obj:`\"main\"`):\n",
      "        model_kwargs:\n",
      "            Additional dictionary of keyword arguments passed along to the model's :obj:`from_pretrained(...,\n",
      "            **model_kwargs)` function.\n",
      "        kwargs:\n",
      "            Additional keyword arguments passed along to the specific pipeline init (see the documentation for the\n",
      "            corresponding pipeline class for possible values).\n",
      "    \n",
      "    Returns:\n",
      "        :class:`~transformers.Pipeline`: A suitable pipeline for the task.\n",
      "    \n",
      "    Examples::\n",
      "    \n",
      "        >>> from transformers import pipeline, AutoModelForTokenClassification, AutoTokenizer\n",
      "    \n",
      "        >>> # Sentiment analysis pipeline\n",
      "        >>> pipeline('sentiment-analysis')\n",
      "    \n",
      "        >>> # Question answering pipeline, specifying the checkpoint identifier\n",
      "        >>> pipeline('question-answering', model='distilbert-base-cased-distilled-squad', tokenizer='bert-base-cased')\n",
      "    \n",
      "        >>> # Named entity recognition pipeline, passing in a specific model and tokenizer\n",
      "        >>> model = AutoModelForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
      "        >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
      "        >>> pipeline('ner', model=model, tokenizer=tokenizer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "present-essay",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998724460601807}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline('sentiment-analysis')\n",
    "\n",
    "p(\"I'm happy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "operational-cooling",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9994174242019653}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(\"I'm sad.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "attended-domain",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9998021125793457}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(\"I'm not happy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "confidential-making",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9996402,\n",
       "  'word': 'Ulrich Glassmann',\n",
       "  'start': 35,\n",
       "  'end': 51},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.8913957,\n",
       "  'word': 'Europa - Universität Flensburg',\n",
       "  'start': 59,\n",
       "  'end': 89},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.988505,\n",
       "  'word': 'EUF',\n",
       "  'start': 92,\n",
       "  'end': 95},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.6957305,\n",
       "  'word': 'Cluster',\n",
       "  'start': 130,\n",
       "  'end': 137},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9996954,\n",
       "  'word': 'Ulrich',\n",
       "  'start': 139,\n",
       "  'end': 145}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "p = pipeline('ner', aggregation_strategy=transformers.pipelines.AggregationStrategy.SIMPLE)\n",
    "\n",
    "p(\"\"\"We would like to belatedly welcome Ulrich Glassmann of the Europa-Universität\n",
    "  Flensburg (#EUF), who is currently a guest at the Cluster. Ulrich has just decided\n",
    "  to extend his stay until the end of June, welcome news indeed!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "environmental-birthday",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wastl/.local/lib/python3.9/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'It is not only different calculations that cause headaches. Self-perception shows that in Germany there are massive misunderstandings about the extent and type of inequality.'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline('translation', model='facebook/wmt19-de-en')\n",
    "\n",
    "p(\"\"\"Nicht nur unterschiedliche Berechnungen bereiten Kopfzerbrechen.\n",
    "  Bei der Eigenwahrnehmung zeigt sich: In Deutschland gibt es massive\n",
    "  Missverständnisse über Ausmaß und Art von Ungleichheit.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adapted-adams",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'Mit Verspätung begrüßen wir Ulrich Glassmann von der Europa-Universität Flensburg (# EUF), der derzeit zu Gast im Cluster ist. Er hat sich gerade entschieden, seinen Aufenthalt bis Ende Juni zu verlängern, eine gute Nachricht!'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline('translation', model='facebook/wmt19-en-de')\n",
    "\n",
    "p(\"\"\"We would like to belatedly welcome Ulrich Glassmann of the Europa-Universität\n",
    "  Flensburg (#EUF), who is currently a guest at the Cluster. Ulrich has just decided\n",
    "  to extend his stay until the end of June, welcome news indeed!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "iraqi-values",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In Germany there are massive misunderstandings about the extent and type of inequality. For instance, the fact that the \"social mobility\" issue is widely used in Germany and abroad suggests that some in Germany feel they have reached greater levels of economic equality without having'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pipeline('text-generation')\n",
    "\n",
    "p(\"In Germany there are massive misunderstandings about the extent and type of inequality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "biological-corruption",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'some in Germany feel they have reached greater levels of economic equality without having to depend on their labor. There are the small cities in particular without one of the very high productivity countries that have been the primary labor of the industrialised world.\\n\\nThey'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p(\"some in Germany feel they have reached greater levels of economic equality without having\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-wednesday",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Transformers can be \"fine-tuned\" to a specific task, see https://huggingface.co/transformers/training.html\n",
    "Adding a task-specific head on top of a pre-trained transformer can overcome the problem of not enough\n",
    "training data. Pre-training unsupervised on large amounts of data On top of the pre-trained model\n",
    "  For example, you want to classify Tweets by their sentiment (positive or negative emotion).\n",
    "  You might be able to manually label 1000s of Tweets for training, however this is by far not sufficient for a model to cope with the vocabularies and variety of natural language.\n",
    "  For example, RoBERTa uses 160 GiB of text for pre-training.\n",
    "\n",
    "- \n",
    "  - potential tasks: text classification (including sentiment analysis), text generation and summarization, question answering, machine translation, etc. Also image classification ...\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
